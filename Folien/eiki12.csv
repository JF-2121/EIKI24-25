"Markov Decision Processes (MDPs)"; What is a Markov Decision Process? | A Markov Decision Process (MDP) is a framework used in reinforcement learning to model decision-making under uncertainty. It consists of states, actions, transitions defined by probabilities and rewards, and a discount factor to weight future rewards over time.
"States and Actions in MDPs"; What are states and actions in the context of an MDP? | States represent the current situation or environment that the agent is in. Actions are the possible choices the agent can make to transition from one state to another, with associated rewards.
"Transitions (P(s’|s,a))"; What do transitions mean in an MDP? | Transitions describe the probability of moving from one state to another given a specific action. They define the stochastic nature of the environment.
"Rewards (R(s,a,s’) and Discount Factor γ)"; What are rewards and how does the discount factor work in an MDP? | Rewards are the outcomes received by the agent when taking an action, which guide the learning process. The discount factor γ determines how much value future rewards have compared to immediate rewards.
"Policy in MDPs"; What is a policy in the context of solving an MDP? | A policy is a rule that defines which action to take from each state. An optimal policy maximizes the expected total reward over time when following the defined actions.
"Solving MDPs: Finding Optimal Policies"; How is an MDP solved to find an optimal policy? | To solve an MDP, one must determine the best action to take from each state so that the cumulative expected reward is maximized. This involves analyzing transitions and rewards while considering the discount factor.
"Applications of MDPs in AI"; What are some applications of Markov Decision Processes in Artificial Intelligence? | MDPs are used in tasks like protein structure prediction, self-driving cars, solving Rubik's Cube with a robot hand, playing Dota 2, achieving grandmaster level in StarCraft II using multi-agent reinforcement learning, and collective robot reinforcement learning.
"Expected Utility Maximization"; How is expected utility maximized in MDPs? | The agent calculates the expected value of taking each action from a given state, considering future transitions and rewards weighted by the discount factor γ. The action with the highest expected value is chosen.
