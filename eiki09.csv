"Motivation für Unsicherheit; Warum ist Unsicherheit ein zentraler Aspekt in der KI?" | Unsicherheit ist in vielen Szenarien allgegenwärtig, z. B. durch unvollständige Daten oder Fehler. KI nutzt Wahrscheinlichkeiten, um trotz Unsicherheiten logische Schlussfolgerungen zu ziehen.
"Kolmogorov-Axiome; Was sind die Grundprinzipien der Wahrscheinlichkeitstheorie?" | 1. Wahrscheinlichkeiten liegen zwischen 0 und 1. 2. Wahrscheinlichkeit eines sicheren Ereignisses ist 1. 3. Wahrscheinlichkeit der Vereinigung disjunkter Ereignisse ist die Summe ihrer Wahrscheinlichkeiten.
"Naïve Bayes Modell; Was ist das Naïve Bayes Modell?" | Ein Modell, das annimmt, dass alle Effekte unabhängig sind, wenn die Ursache bekannt ist. Es ist effizient und skaliert linear mit der Anzahl der Variablen.
"Bayesian Networks; Was sind Bayesianische Netzwerke?" | Grafische Modelle, die abhängige Zufallsvariablen als gerichtete azyklische Graphen (DAGs) darstellen. Sie nutzen bedingte Unabhängigkeiten, um komplexe Wahrscheinlichkeiten zu vereinfachen.
"Komponenten eines Bayesian Networks; Welche Komponenten hat ein Bayesian Network?" | 1. Knoten: Zufallsvariablen. 2. Kanten: Direkte Abhängigkeiten. 3. Bedingte Wahrscheinlichkeitsverteilungen (CPTs) für jeden Knoten.
"Unabhängigkeit in BNs; Wann sind Variablen unabhängig?" | Eine Variable ist unabhängig von allen nicht-abgeleiteten Knoten, wenn ihre Eltern bekannt sind (Local Markov Assumption).
"Bedingte Unabhängigkeit; Was ist bedingte Unabhängigkeit?" | Zwei Variablen \\(X\\) und \\(Y\\) sind unabhängig, wenn ein dritter Zustand \\(Z\\) bekannt ist, der ihre Beziehung beschreibt. Beispiel: Alter und Geschlecht sind unabhängig, wenn Rauchen bekannt ist.
"Variable Elimination; Was ist Variable Elimination?" | Ein Algorithmus zur Inferenz in Bayesianischen Netzwerken, der Variablen schrittweise eliminiert, um Wahrscheinlichkeiten effizient zu berechnen.
"Markov Blanket; Was ist eine Markov-Decke?" | Die Markov-Decke einer Variablen umfasst deren Eltern, Kinder und die Eltern ihrer Kinder. Sie enthält alle Informationen, die zur Bestimmung der Variablen erforderlich sind.
"Gibbs Sampling; Was ist Gibbs Sampling?" | Ein Algorithmus zur Approximation posteriorer Wahrscheinlichkeiten. Er nutzt eine Markov-Kette, die sukzessive Zustände erzeugt, um die Verteilung zu approximieren.
"Wahrscheinlichkeitsabschätzung; Wie kann man Wahrscheinlichkeiten durch Sampling schätzen?" | Durch wiederholtes Sampling eines Bayesianischen Netzwerks und Zählen der Ergebnisse kann die Wahrscheinlichkeit von Zuständen abgeschätzt werden.
"Rejection Sampling; Was ist Rejection Sampling?" | Ein Verfahren, bei dem unpassende Proben, die nicht mit bekannten Beweisen übereinstimmen, verworfen werden, um die Schätzung zu verbessern.
"Approximate Inference; Was sind Approximationstechniken in Bayesianischen Netzwerken?" | 1. Rejection Sampling, 2. Likelihood Weighting, 3. Gibbs Sampling, 4. Markov Chain Monte Carlo (MCMC).
"Komplexität der Inferenz; Warum ist Inferenz in BNs schwierig?" | Inferenz (sogar Näherung) in Bayesianischen Netzwerken ist NP-schwer, da die Berechnung der Wahrscheinlichkeiten exponentiell mit der Anzahl der Variablen wächst.
"Zusammenfassung; Was sind die Hauptpunkte zu Unsicherheit und Bayesianischen Netzwerken?" | 1. Wahrscheinlichkeiten modellieren Unsicherheit. 2. Bayesianische Netzwerke sind kompakte Modelle. 3. Inferenz ist komplex, aber lösbar mit Techniken wie Variable Elimination und Sampling.
